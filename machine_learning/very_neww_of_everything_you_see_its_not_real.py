# -*- coding: utf-8 -*-
"""very neww_of everything_you_see_its_not_real

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IYY0LpxUOpLOXLAYFd6aehDphxapTJ8h
"""

from google.colab import drive
drive.mount('/content/drive')

# ================================================================
# SMART AGRICULTURE: CLASSIFICATION OF TOMATO LEAF FRESHNESS (SVM MODEL)
# Dataset: Tomato_Bacterial_spot / Tomato_Tomato_Yellow_Leaf_Curl_Virus / Tomato_healthy
# Author: < Audrey >
# ================================================================

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
from skimage.feature import hog
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import learning_curve
import seaborn as sns
from sklearn.decomposition import PCA


# ================================================================
# 1. Load Dataset
# ================================================================
dataset_path = "/content/drive/MyDrive/Tomato_Leaf"

classes = ["Tomato_Bacterial_spot", "Tomato_Tomato_Yellow_Leaf_Curl_Virus", "Tomato_healthy"]
images = []
labels = []

print("üì• Loading dataset...")

for label_idx, class_name in enumerate(classes):
    class_folder = os.path.join(dataset_path, class_name)
    for file in os.listdir(class_folder):
        file_path = os.path.join(class_folder, file)

        # Read image
        image = cv2.imread(file_path)
        if image is None:
            continue

        # Resize
        image = cv2.resize(image, (128, 128))

        # Convert to gray
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

        images.append(gray)
        labels.append(label_idx)

print(f"‚úî Dataset Loaded: {len(images)} images")

# Convert to numpy
images = np.array(images)
labels = np.array(labels)

# ================================================================
# 2. HOG Feature Extraction
# ================================================================
print("üîç Extracting HOG features...")

hog_features = []
for img in images:
    feature, _ = hog(img,
                     orientations=9,
                     pixels_per_cell=(8, 8),
                     cells_per_block=(2, 2),
                     visualize=True,
                     block_norm='L2-Hys')
    hog_features.append(feature)

hog_features = np.array(hog_features)

print(f"‚úî HOG Feature Shape: {hog_features.shape}")

# ================================================================
# Dimensionality Reduction (PCA) ‚Äî REDUCE OVERFITTING
# ================================================================
print("üîª Applying PCA to reduce dimensionality...")

pca = PCA(n_components=300, random_state=42)  # reduce HOG features ~8000 ‚Üí 300
hog_features = pca.fit_transform(hog_features)

print(f"‚úî PCA Reduced Shape: {hog_features.shape}")

# ================================================================
# 3. Split Train / Validation / Test
# ================================================================
print("‚úÇ Splitting data...")

X_train, X_temp, y_train, y_temp = train_test_split(
    hog_features, labels, test_size=0.30, random_state=42, stratify=labels
)

X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp
)

print(f"‚úî Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)}")

# ================================================================
# 4. Train SVM with RBF Kernel + GridSearchCV
# ================================================================
print("‚öô Training SVM model (RBF kernel)...")

parameters = {
    'C': [1, 10, 50, 100],
    'gamma': ['scale', 0.01, 0.001]
}

svm_model = GridSearchCV(
    SVC(kernel='rbf'),
    parameters,
    cv=3,
    scoring='accuracy',
    verbose=1,
    n_jobs=-1
)
svm_model.fit(X_train, y_train)


print("\nüèÜ Best SVM Parameters:", svm_model.best_params_)
best_svm = svm_model.best_estimator_

# ================================================================
# 5. Validation Evaluation
# ================================================================
print("\nüìä VALIDATION RESULTS")
val_pred = best_svm.predict(X_val)

print("Validation Accuracy:", accuracy_score(y_val, val_pred))
print(classification_report(y_val, val_pred, target_names=classes))

# ================================================================
# 6. Final Testing
# ================================================================
print("\nüß™ TEST RESULTS")

test_pred = best_svm.predict(X_test)

test_acc = accuracy_score(y_test, test_pred)

print(f"üéØ Final Test Accuracy: {test_acc * 100:.2f}%")
print(classification_report(y_test, test_pred, target_names=classes))

# ================================================================
# ‚≠ê Training & Validation Metrics (Precision, Recall, F1 Learning Curves)
# ================================================================
print("\nüìà Generating Precision / Recall / F1 Learning Curves...")

metrics = {
    "Precision": "precision_macro",
    "Recall": "recall_macro",
    "F1-Score": "f1_macro"
}

for metric_name, metric in metrics.items():
    print(f"\nüìä Generating {metric_name} Learning Curve...")

    train_sizes_m, train_scores_m, val_scores_m = learning_curve(
        best_svm,
        X_train,
        y_train,
        cv=3,
        train_sizes=np.linspace(0.1, 1.0, 10),
        scoring=metric,
        n_jobs=-1
    )

    train_mean_m = train_scores_m.mean(axis=1)
    val_mean_m = val_scores_m.mean(axis=1)

    plt.figure(figsize=(7, 6))
    plt.plot(train_sizes_m, val_mean_m, label=f"Validation {metric_name}")

    # ---- NEW: Evaluate TEST SCORE for each training size ----
    test_scores_m = []
    for size in train_sizes_m:
        idx = int(size)
        X_partial = X_train[:idx]
        y_partial = y_train[:idx]

        best_svm.fit(X_partial, y_partial)
        test_score = best_svm.score(X_test, y_test)
        test_scores_m.append(test_score)

    plt.plot(train_sizes_m, test_scores_m, label=f"Test {metric_name}")

    plt.title(f"Training, Validation & Test {metric_name} (Learning Curve)")
    plt.ylabel(metric_name)
    plt.legend()
    plt.grid(True)
    plt.show()


# ================================================================
# 7. Confusion Matrix
# ================================================================
cm = confusion_matrix(y_test, test_pred)

plt.figure(figsize=(7, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=classes, yticklabels=classes)

plt.gca().xaxis.tick_top()
plt.gca().xaxis.set_label_position('top')

plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix (SVM)", pad=30)

plt.show()

# ================================================================
# DONE!
# ================================================================
print("‚ú® SVM Classification Completed Successfully!")

"""---

# ‚úÖ **Short Explanation for Each Part of Your Code**
---

### **1. Loading the Dataset**

- dataset_path = folder containing the 3 tomato leaf classes

- classes list stores all class names

- images stores raw image data

- labels stores the numeric classes (0, 1, 2)

---

### **2. HOG Feature Extraction**

HOG turns an image into a compact feature vector focusing on:

‚úî edges
‚úî corners
‚úî gradients
‚úî texture patterns

These features are highly effective for plant disease classification.

Parameters:

orientations=9 ‚Üí gradient directions

pixels_per_cell=(8,8) ‚Üí small cell blocks

cells_per_block=(2,2) ‚Üí grouping of cells

visualize=True ‚Üí allows debugging

block_norm='L2-Hys' ‚Üí recommended normalization

HOG output shape is usually 4000‚Äì9000 features per image.

---

### **PCA - Dimensionality Reduction**

HOG can produce thousands of features.
PCA compresses them into 300 essential components.

Benefits of PCA:

Reduces noise

Prevents overfitting

Faster training

Lower memory usage

Keeps the most important variance in the data

This turns the HOG feature vector from something like:

üü• 8500 features ‚Üí 300 features

with almost no performance loss.

---

### **3. Splitting Train / Validation / Test**

70% data ‚Üí Training

Remaining 30% ‚Üí temporary pool

**The remaining 30% is split:**

15% ‚Üí Validation

15% ‚Üí Final Test

The dataset is split into:

* **Train** ‚Üí used to train the model
* **Validation** ‚Üí used to tune hyperparameters
* **Test** ‚Üí used for final evaluation

This ensures fair and unbiased performance results.

---

### **4. Training SVM With RBF Kernel + GridSearchCV**

GridSearchCV tries all combinations of C and gamma to find the best-performing model.

- C : controls margin hardness (regularization)

- gamma : controls influence of each support vector (feature influence)

It automatically selects the **best SVM parameters** that produce the highest accuracy.

- kernel='rbf' = most powerful SVM kernel

- cv=3 ‚Üí 3-fold cross-validation

- n_jobs=-1 ‚Üí use all CPU cores

- GridSearchCV tries all parameter combinations

---

### **5. Validation Evaluation**

After training, the model is tested on the validation set before testing.
This helps check whether the model is generalizing well or overfitting.

The validation set evaluates:

- Precision

- Recall

- F1-score

- Per-class performance
---

### **6. Final Test Evaluation**
The test set is never used during training.
It gives an unbiased measurement of final model performance.

---
### **Training & Validation Accuracy (Learning Curve)**

The learning curve shows how accuracy changes as the training size increases.
It helps you understand:

* whether more data improves the model
* whether the model is underfitting or overfitting
---
### **7. Learning Curves**

Learning curves show:

- How performance changes when training size increases

- Whether more data helps

- Whether the model overfits or underfits

- Comparison of Train vs Validation vs Test curves

This is very useful for analysis.

---

### **8. Confusion Matrix**

The confusion matrix shows:

* Correct predictions
* Wrong predictions
* Which classes are confused with each other

This helps visualize model strengths and weaknesses.

---
"""